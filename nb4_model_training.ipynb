{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Evaluation\n",
    "# x_train -  train samples: 2185\n",
    "# x_test - testing samples: 547\n",
    "# categories train: 2185\n",
    "# total samples\n",
    "\n",
    "# Retrieve previously stored variables\n",
    "%store -r x_train \n",
    "%store -r x_test \n",
    "%store -r y_train\n",
    "%store -r y_test\n",
    "%store -r yy\n",
    "%store -r le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-121.22656   ,   65.431366  ,  -32.428158  ,   34.480045  ,\n",
       "        -20.285666  ,    6.766245  ,  -16.782026  ,   18.733065  ,\n",
       "        -13.358098  ,   11.913208  ,  -13.08202   ,   14.028469  ,\n",
       "        -17.593868  ,   13.480655  ,   -7.664562  ,    2.072071  ,\n",
       "         -2.1378267 ,    3.0446467 ,   -3.9934742 ,   10.67374   ,\n",
       "        -13.127217  ,   11.37405   ,    2.145628  ,   -1.9967248 ,\n",
       "         -0.800105  ,    3.9523966 ,   -4.335793  ,    2.2346418 ,\n",
       "         -2.9265692 ,    1.1134802 ,   -6.3341794 ,    0.1592586 ,\n",
       "         -4.7829237 ,   -2.4864097 ,   -5.620346  ,    4.3450346 ,\n",
       "         -9.871351  ,    2.3727033 ,   -0.61368215,    3.178931  ],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "(1346, 40)\n"
     ]
    }
   ],
   "source": [
    "display(x_train[1])\n",
    "print((y_train))\n",
    "display((yy.shape[1]))\n",
    "print(len(x_test))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "# Extract the number of labels - 4 in our case\n",
    "num_labels = yy.shape[1]\n",
    "\n",
    "# Begin constructing ML model\n",
    "# Create an object of the Sequential class\n",
    "model = Sequential()\n",
    "\n",
    "# Create input layer using the Dense function\n",
    "numNodes = 128; # Trial and error - 1/4 of 256 bit - went from 64 to 32 because size dropped from 2.7k to 1.4k\n",
    "numMFCC = 40;\n",
    "# Input shape is the size of the input array (1-D array of 40 columns, 1 row)\n",
    "model.add(Dense(numNodes, input_shape=(numMFCC,)));\n",
    "# Specifying the activation function to be used - relu: Rectified Linear Activiation\n",
    "model.add(Activation('relu'))\n",
    "# Dropout value of 50% - means random half of neurons exluded from each update cycle. Used to prevent overfitting.\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Add subsequest hidden layer - DENSE function\n",
    "model.add(Dense(numNodes))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add the output layer - DENSE function\n",
    "# Output nodes is the different categories\n",
    "# Different usage of activation function\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# Optimizer \"adam\" is a typical optimizer used - variation SGD (stochastic gradient descent)\n",
    "# SGD utilizes the gradient of the loss function with respects to the weight\n",
    "# loss -> typical loss function \n",
    "# metrics is output to be displayed (accuracy is the output of the loss function (?))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               5248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 22,276\n",
      "Trainable params: 22,276\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Pre-training accuracy: 35.3333%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[18.44668312072754, 0.35333332419395447]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "# Verbose - displays info if desired (verbose = 0 means silent, just print accuracy value)\n",
    "# evaluate returns loss value and score value\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "# Accuracy - the metrics value evaluated based on loss function\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)\n",
    "\n",
    "display(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1346 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "1346/1346 [==============================] - 1s 463us/step - loss: 20.0016 - accuracy: 0.3462 - val_loss: 1.5627 - val_accuracy: 0.6067\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.56267, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 2/100\n",
      "1346/1346 [==============================] - 0s 225us/step - loss: 6.4284 - accuracy: 0.4227 - val_loss: 0.8769 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.56267 to 0.87693, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 3/100\n",
      "1346/1346 [==============================] - 0s 288us/step - loss: 2.8673 - accuracy: 0.4814 - val_loss: 0.8573 - val_accuracy: 0.6133\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.87693 to 0.85727, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 4/100\n",
      "1346/1346 [==============================] - 0s 262us/step - loss: 1.7817 - accuracy: 0.5371 - val_loss: 0.8479 - val_accuracy: 0.6533\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.85727 to 0.84789, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 5/100\n",
      "1346/1346 [==============================] - 0s 300us/step - loss: 1.4227 - accuracy: 0.5364 - val_loss: 0.8630 - val_accuracy: 0.6267\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.84789\n",
      "Epoch 6/100\n",
      "1346/1346 [==============================] - 0s 274us/step - loss: 1.1547 - accuracy: 0.5654 - val_loss: 0.8410 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.84789 to 0.84105, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 7/100\n",
      "1346/1346 [==============================] - 0s 235us/step - loss: 1.0976 - accuracy: 0.5944 - val_loss: 0.7769 - val_accuracy: 0.7333\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.84105 to 0.77692, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 8/100\n",
      "1346/1346 [==============================] - 0s 268us/step - loss: 1.0304 - accuracy: 0.5936 - val_loss: 0.8084 - val_accuracy: 0.6467\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.77692\n",
      "Epoch 9/100\n",
      "1346/1346 [==============================] - 0s 224us/step - loss: 0.9722 - accuracy: 0.6122 - val_loss: 0.7600 - val_accuracy: 0.6933\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.77692 to 0.75997, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 10/100\n",
      "1346/1346 [==============================] - 0s 261us/step - loss: 0.8487 - accuracy: 0.6434 - val_loss: 0.7295 - val_accuracy: 0.7333\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.75997 to 0.72954, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 11/100\n",
      "1346/1346 [==============================] - 0s 226us/step - loss: 0.8569 - accuracy: 0.6352 - val_loss: 0.7076 - val_accuracy: 0.6867\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.72954 to 0.70762, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 12/100\n",
      "1346/1346 [==============================] - 0s 267us/step - loss: 0.8101 - accuracy: 0.6612 - val_loss: 0.6689 - val_accuracy: 0.7333\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.70762 to 0.66893, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 13/100\n",
      "1346/1346 [==============================] - 0s 274us/step - loss: 0.7647 - accuracy: 0.6947 - val_loss: 0.6462 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.66893 to 0.64622, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 14/100\n",
      "1346/1346 [==============================] - 0s 236us/step - loss: 0.7795 - accuracy: 0.6813 - val_loss: 0.6290 - val_accuracy: 0.7867\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.64622 to 0.62900, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 15/100\n",
      "1346/1346 [==============================] - 0s 230us/step - loss: 0.6941 - accuracy: 0.6939 - val_loss: 0.5898 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.62900 to 0.58978, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 16/100\n",
      "1346/1346 [==============================] - 0s 231us/step - loss: 0.6914 - accuracy: 0.7058 - val_loss: 0.5985 - val_accuracy: 0.7933\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.58978\n",
      "Epoch 17/100\n",
      "1346/1346 [==============================] - 0s 231us/step - loss: 0.6864 - accuracy: 0.7117 - val_loss: 0.6040 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.58978\n",
      "Epoch 18/100\n",
      "1346/1346 [==============================] - 0s 257us/step - loss: 0.6321 - accuracy: 0.7355 - val_loss: 0.5770 - val_accuracy: 0.7867\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.58978 to 0.57695, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 19/100\n",
      "1346/1346 [==============================] - 0s 224us/step - loss: 0.6286 - accuracy: 0.7377 - val_loss: 0.5466 - val_accuracy: 0.8067\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.57695 to 0.54660, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 20/100\n",
      "1346/1346 [==============================] - 0s 244us/step - loss: 0.6005 - accuracy: 0.7385 - val_loss: 0.5419 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.54660 to 0.54186, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 21/100\n",
      "1346/1346 [==============================] - 0s 263us/step - loss: 0.5833 - accuracy: 0.7578 - val_loss: 0.5437 - val_accuracy: 0.8067\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.54186\n",
      "Epoch 22/100\n",
      "1346/1346 [==============================] - 0s 225us/step - loss: 0.5782 - accuracy: 0.7578 - val_loss: 0.5331 - val_accuracy: 0.8133\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.54186 to 0.53313, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 23/100\n",
      "1346/1346 [==============================] - 0s 240us/step - loss: 0.5573 - accuracy: 0.7615 - val_loss: 0.5581 - val_accuracy: 0.7933\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.53313\n",
      "Epoch 24/100\n",
      "1346/1346 [==============================] - 0s 359us/step - loss: 0.5446 - accuracy: 0.7660 - val_loss: 0.5230 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.53313 to 0.52302, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 25/100\n",
      "1346/1346 [==============================] - 1s 677us/step - loss: 0.5320 - accuracy: 0.7749 - val_loss: 0.5299 - val_accuracy: 0.8133\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.52302\n",
      "Epoch 26/100\n",
      "1346/1346 [==============================] - 0s 250us/step - loss: 0.5254 - accuracy: 0.7727 - val_loss: 0.4913 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.52302 to 0.49126, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 27/100\n",
      "1346/1346 [==============================] - 0s 364us/step - loss: 0.5294 - accuracy: 0.7927 - val_loss: 0.5136 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.49126\n",
      "Epoch 28/100\n",
      "1346/1346 [==============================] - 1s 398us/step - loss: 0.4951 - accuracy: 0.7957 - val_loss: 0.4954 - val_accuracy: 0.8267\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.49126\n",
      "Epoch 29/100\n",
      "1346/1346 [==============================] - 0s 304us/step - loss: 0.4715 - accuracy: 0.7964 - val_loss: 0.4826 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.49126 to 0.48263, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 30/100\n",
      "1346/1346 [==============================] - 0s 326us/step - loss: 0.4596 - accuracy: 0.7979 - val_loss: 0.4848 - val_accuracy: 0.8467\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.48263\n",
      "Epoch 31/100\n",
      "1346/1346 [==============================] - 0s 344us/step - loss: 0.4716 - accuracy: 0.8016 - val_loss: 0.4936 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.48263\n",
      "Epoch 32/100\n",
      "1346/1346 [==============================] - 0s 233us/step - loss: 0.4288 - accuracy: 0.8143 - val_loss: 0.4927 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.48263\n",
      "Epoch 33/100\n",
      "1346/1346 [==============================] - 1s 409us/step - loss: 0.4621 - accuracy: 0.8113 - val_loss: 0.5072 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.48263\n",
      "Epoch 34/100\n",
      "1346/1346 [==============================] - 1s 403us/step - loss: 0.4325 - accuracy: 0.8143 - val_loss: 0.5002 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.48263\n",
      "Epoch 35/100\n",
      "1346/1346 [==============================] - 1s 496us/step - loss: 0.4620 - accuracy: 0.8210 - val_loss: 0.4724 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.48263 to 0.47239, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 36/100\n",
      "1346/1346 [==============================] - 0s 358us/step - loss: 0.4238 - accuracy: 0.8358 - val_loss: 0.5130 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47239\n",
      "Epoch 37/100\n",
      "1346/1346 [==============================] - 1s 438us/step - loss: 0.4216 - accuracy: 0.8291 - val_loss: 0.4384 - val_accuracy: 0.8533\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.47239 to 0.43835, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 38/100\n",
      "1346/1346 [==============================] - 0s 317us/step - loss: 0.3847 - accuracy: 0.8388 - val_loss: 0.5165 - val_accuracy: 0.8467\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.43835\n",
      "Epoch 39/100\n",
      "1346/1346 [==============================] - 0s 291us/step - loss: 0.4188 - accuracy: 0.8269 - val_loss: 0.4341 - val_accuracy: 0.8267\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.43835 to 0.43414, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 40/100\n",
      "1346/1346 [==============================] - 0s 296us/step - loss: 0.4084 - accuracy: 0.8373 - val_loss: 0.4354 - val_accuracy: 0.8467\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.43414\n",
      "Epoch 41/100\n",
      "1346/1346 [==============================] - 1s 462us/step - loss: 0.3665 - accuracy: 0.8574 - val_loss: 0.4299 - val_accuracy: 0.8533\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.43414 to 0.42991, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 42/100\n",
      "1346/1346 [==============================] - 1s 386us/step - loss: 0.3706 - accuracy: 0.8536 - val_loss: 0.4317 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.42991\n",
      "Epoch 43/100\n",
      "1346/1346 [==============================] - 1s 418us/step - loss: 0.3796 - accuracy: 0.8447 - val_loss: 0.4343 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.42991\n",
      "Epoch 44/100\n",
      "1346/1346 [==============================] - 1s 466us/step - loss: 0.3597 - accuracy: 0.8529 - val_loss: 0.4603 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.42991\n",
      "Epoch 45/100\n",
      "1346/1346 [==============================] - 0s 277us/step - loss: 0.3690 - accuracy: 0.8529 - val_loss: 0.4151 - val_accuracy: 0.8533\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.42991 to 0.41515, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 46/100\n",
      "1346/1346 [==============================] - 1s 555us/step - loss: 0.3658 - accuracy: 0.8655 - val_loss: 0.4958 - val_accuracy: 0.8467\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.41515\n",
      "Epoch 47/100\n",
      "1346/1346 [==============================] - 1s 477us/step - loss: 0.3347 - accuracy: 0.8618 - val_loss: 0.4505 - val_accuracy: 0.8533\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.41515\n",
      "Epoch 48/100\n",
      "1346/1346 [==============================] - 1s 411us/step - loss: 0.3438 - accuracy: 0.8670 - val_loss: 0.4224 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.41515\n",
      "Epoch 49/100\n",
      "1346/1346 [==============================] - 0s 310us/step - loss: 0.3604 - accuracy: 0.8603 - val_loss: 0.4692 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.41515\n",
      "Epoch 50/100\n",
      "1346/1346 [==============================] - 0s 362us/step - loss: 0.3665 - accuracy: 0.8566 - val_loss: 0.4473 - val_accuracy: 0.8467\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.41515\n",
      "Epoch 51/100\n",
      "1346/1346 [==============================] - 1s 437us/step - loss: 0.3264 - accuracy: 0.8715 - val_loss: 0.5079 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.41515\n",
      "Epoch 52/100\n",
      "1346/1346 [==============================] - 1s 432us/step - loss: 0.3622 - accuracy: 0.8514 - val_loss: 0.4865 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.41515\n",
      "Epoch 53/100\n",
      "1346/1346 [==============================] - 1s 440us/step - loss: 0.3156 - accuracy: 0.8819 - val_loss: 0.5654 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.41515\n",
      "Epoch 54/100\n",
      "1346/1346 [==============================] - 0s 276us/step - loss: 0.3375 - accuracy: 0.8655 - val_loss: 0.4701 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.41515\n",
      "Epoch 55/100\n",
      "1346/1346 [==============================] - 1s 389us/step - loss: 0.3433 - accuracy: 0.8581 - val_loss: 0.4642 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.41515\n",
      "Epoch 56/100\n",
      "1346/1346 [==============================] - 1s 382us/step - loss: 0.3072 - accuracy: 0.8730 - val_loss: 0.4985 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.41515\n",
      "Epoch 57/100\n",
      "1346/1346 [==============================] - 0s 286us/step - loss: 0.3566 - accuracy: 0.8663 - val_loss: 0.4486 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.41515\n",
      "Epoch 58/100\n",
      "1346/1346 [==============================] - 0s 358us/step - loss: 0.3327 - accuracy: 0.8678 - val_loss: 0.5209 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.41515\n",
      "Epoch 59/100\n",
      "1346/1346 [==============================] - 0s 298us/step - loss: 0.3386 - accuracy: 0.8715 - val_loss: 0.5288 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.41515\n",
      "Epoch 60/100\n",
      "1346/1346 [==============================] - 0s 289us/step - loss: 0.2923 - accuracy: 0.8863 - val_loss: 0.4659 - val_accuracy: 0.8533\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.41515\n",
      "Epoch 61/100\n",
      "1346/1346 [==============================] - 0s 343us/step - loss: 0.3348 - accuracy: 0.8730 - val_loss: 0.4402 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.41515\n",
      "Epoch 62/100\n",
      "1346/1346 [==============================] - 0s 295us/step - loss: 0.3110 - accuracy: 0.8826 - val_loss: 0.4196 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.41515\n",
      "Epoch 63/100\n",
      "1346/1346 [==============================] - 1s 468us/step - loss: 0.3020 - accuracy: 0.8826 - val_loss: 0.5231 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.41515\n",
      "Epoch 64/100\n",
      "1346/1346 [==============================] - 0s 355us/step - loss: 0.2924 - accuracy: 0.8811 - val_loss: 0.5251 - val_accuracy: 0.8533\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.41515\n",
      "Epoch 65/100\n",
      "1346/1346 [==============================] - 1s 403us/step - loss: 0.3516 - accuracy: 0.8633 - val_loss: 0.4571 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.41515\n",
      "Epoch 66/100\n",
      "1346/1346 [==============================] - 0s 306us/step - loss: 0.3139 - accuracy: 0.8819 - val_loss: 0.4411 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.41515\n",
      "Epoch 67/100\n",
      "1346/1346 [==============================] - 1s 417us/step - loss: 0.3225 - accuracy: 0.8752 - val_loss: 0.4796 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.41515\n",
      "Epoch 68/100\n",
      "1346/1346 [==============================] - 0s 301us/step - loss: 0.2596 - accuracy: 0.8975 - val_loss: 0.4381 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.41515\n",
      "Epoch 69/100\n",
      "1346/1346 [==============================] - 0s 345us/step - loss: 0.2789 - accuracy: 0.8819 - val_loss: 0.4609 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.41515\n",
      "Epoch 70/100\n",
      "1346/1346 [==============================] - 1s 401us/step - loss: 0.3050 - accuracy: 0.8930 - val_loss: 0.4917 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.41515\n",
      "Epoch 71/100\n",
      "1346/1346 [==============================] - 0s 271us/step - loss: 0.2385 - accuracy: 0.9004 - val_loss: 0.5206 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.41515\n",
      "Epoch 72/100\n",
      "1346/1346 [==============================] - 1s 403us/step - loss: 0.2786 - accuracy: 0.9064 - val_loss: 0.5666 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.41515\n",
      "Epoch 73/100\n",
      "1346/1346 [==============================] - 0s 311us/step - loss: 0.3006 - accuracy: 0.8871 - val_loss: 0.4952 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.41515\n",
      "Epoch 74/100\n",
      "1346/1346 [==============================] - 1s 432us/step - loss: 0.2676 - accuracy: 0.8930 - val_loss: 0.5905 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.41515\n",
      "Epoch 75/100\n",
      "1346/1346 [==============================] - 0s 293us/step - loss: 0.2686 - accuracy: 0.8878 - val_loss: 0.5740 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.41515\n",
      "Epoch 76/100\n",
      "1346/1346 [==============================] - 1s 438us/step - loss: 0.2866 - accuracy: 0.8908 - val_loss: 0.5371 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.41515\n",
      "Epoch 77/100\n",
      "1346/1346 [==============================] - 0s 283us/step - loss: 0.2970 - accuracy: 0.8893 - val_loss: 0.5098 - val_accuracy: 0.9133\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.41515\n",
      "Epoch 78/100\n",
      "1346/1346 [==============================] - 1s 445us/step - loss: 0.3014 - accuracy: 0.8967 - val_loss: 0.5115 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.41515\n",
      "Epoch 79/100\n",
      "1346/1346 [==============================] - 0s 342us/step - loss: 0.2470 - accuracy: 0.9004 - val_loss: 0.5557 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.41515\n",
      "Epoch 80/100\n",
      "1346/1346 [==============================] - 0s 308us/step - loss: 0.2837 - accuracy: 0.8900 - val_loss: 0.6833 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.41515\n",
      "Epoch 81/100\n",
      "1346/1346 [==============================] - 0s 322us/step - loss: 0.2836 - accuracy: 0.8908 - val_loss: 0.6480 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.41515\n",
      "Epoch 82/100\n",
      "1346/1346 [==============================] - 0s 274us/step - loss: 0.2860 - accuracy: 0.8900 - val_loss: 0.5162 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.41515\n",
      "Epoch 83/100\n",
      "1346/1346 [==============================] - 0s 299us/step - loss: 0.2961 - accuracy: 0.8774 - val_loss: 0.5046 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.41515\n",
      "Epoch 84/100\n",
      "1346/1346 [==============================] - 0s 347us/step - loss: 0.2693 - accuracy: 0.8908 - val_loss: 0.5467 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.41515\n",
      "Epoch 85/100\n",
      "1346/1346 [==============================] - 1s 400us/step - loss: 0.2392 - accuracy: 0.9108 - val_loss: 0.6111 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.41515\n",
      "Epoch 86/100\n",
      "1346/1346 [==============================] - 0s 263us/step - loss: 0.2587 - accuracy: 0.8997 - val_loss: 0.5377 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.41515\n",
      "Epoch 87/100\n",
      "1346/1346 [==============================] - 1s 403us/step - loss: 0.2800 - accuracy: 0.8967 - val_loss: 0.5495 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.41515\n",
      "Epoch 88/100\n",
      "1346/1346 [==============================] - 0s 343us/step - loss: 0.2452 - accuracy: 0.9138 - val_loss: 0.6419 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.41515\n",
      "Epoch 89/100\n",
      "1346/1346 [==============================] - 0s 296us/step - loss: 0.3083 - accuracy: 0.8945 - val_loss: 0.4856 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.41515\n",
      "Epoch 90/100\n",
      "1346/1346 [==============================] - 0s 322us/step - loss: 0.2137 - accuracy: 0.9279 - val_loss: 0.5382 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.41515\n",
      "Epoch 91/100\n",
      "1346/1346 [==============================] - 0s 271us/step - loss: 0.2581 - accuracy: 0.9086 - val_loss: 0.5491 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.41515\n",
      "Epoch 92/100\n",
      "1346/1346 [==============================] - 1s 383us/step - loss: 0.2477 - accuracy: 0.9146 - val_loss: 0.4876 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.41515\n",
      "Epoch 93/100\n",
      "1346/1346 [==============================] - 0s 364us/step - loss: 0.2935 - accuracy: 0.8923 - val_loss: 0.6429 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.41515\n",
      "Epoch 94/100\n",
      "1346/1346 [==============================] - 1s 448us/step - loss: 0.2368 - accuracy: 0.9220 - val_loss: 0.5867 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.41515\n",
      "Epoch 95/100\n",
      "1346/1346 [==============================] - 1s 381us/step - loss: 0.2454 - accuracy: 0.9094 - val_loss: 0.5771 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.41515\n",
      "Epoch 96/100\n",
      "1346/1346 [==============================] - 0s 319us/step - loss: 0.2808 - accuracy: 0.8819 - val_loss: 0.5452 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.41515\n",
      "Epoch 97/100\n",
      "1346/1346 [==============================] - 1s 390us/step - loss: 0.2462 - accuracy: 0.8960 - val_loss: 0.5318 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.41515\n",
      "Epoch 98/100\n",
      "1346/1346 [==============================] - 0s 356us/step - loss: 0.2934 - accuracy: 0.8982 - val_loss: 0.4565 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.41515\n",
      "Epoch 99/100\n",
      "1346/1346 [==============================] - 0s 264us/step - loss: 0.2506 - accuracy: 0.9116 - val_loss: 0.4834 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.41515\n",
      "Epoch 100/100\n",
      "1346/1346 [==============================] - 1s 420us/step - loss: 0.2631 - accuracy: 0.9094 - val_loss: 0.5558 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.41515\n",
      "Training completed in time:  0:00:48.145084\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 8 # Arbitrarily chose the value 8\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# Train the model for a fix number of epochs\n",
    "# validation_data - data to evaluate the loss at the end of each epoch\n",
    "# callbacks - display ModelCheckpoint\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  96.73105478286743 %\n",
      "Testing Accuracy:  88.66666555404663 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "\n",
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1]*100, \"%\")\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1]*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Creating a function that extracts the MFCC features of an audio file\n",
    "def extract_feature(file_name):\n",
    "   \n",
    "    try:\n",
    "        audio_data, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40)\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file)\n",
    "        return None, None\n",
    "\n",
    "    return np.array([mfccsscaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that will \n",
    "def print_prediction(file_name):\n",
    "    \n",
    "    # MFCCs of the specifc file contained in prediction_feature\n",
    "    prediction_feature = extract_feature(file_name)\n",
    "\n",
    "    # Directly maps to output\n",
    "    predicted_vector = model.predict_classes(prediction_feature)\n",
    "    \n",
    "    # Inverse transform is used to convert encoded LabelEncoder() values back to strings\n",
    "    predicted_class = le.inverse_transform(predicted_vector) \n",
    "    print(\"The predicted class is:\", predicted_class[0], '\\n') \n",
    "\n",
    "    # Given new instance, model return probability (of belonging to each class) between 0 and 1 \n",
    "    predicted_proba_vector = model.predict_proba(prediction_feature) \n",
    "    # Extract first array from array of arrays\n",
    "    predicted_proba = predicted_proba_vector[0]\n",
    "    \n",
    "    for i in range(len(predicted_proba)): \n",
    "        category = le.inverse_transform(np.array([i]))\n",
    "        print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: dog_bark \n",
      "\n",
      "car_horn \t\t :  0.00000000001011129461409021601526\n",
      "dog_bark \t\t :  0.99999952316284179687500000000000\n",
      "gun_shot \t\t :  0.00000045024393102721660397946835\n",
      "siren \t\t :  0.00000000071334588236382501236221\n",
      "The predicted class is: siren \n",
      "\n",
      "car_horn \t\t :  0.00000000000008221627046701984742\n",
      "dog_bark \t\t :  0.00000000004464676892079744163766\n",
      "gun_shot \t\t :  0.00000000000000000069769081263304\n",
      "siren \t\t :  1.00000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "# Validation \n",
    "import os\n",
    "\n",
    "# Random dog bark file\n",
    "cwd = os.getcwd()\n",
    "filename = cwd + '//UrbanSound8K//audio//fold1//101415-3-0-2.wav'\n",
    "print_prediction(filename)\n",
    "\n",
    "# Siren file\n",
    "cwd = os.getcwd()\n",
    "filename = cwd + '//UrbanSound8K//audio//fold3//184623-8-0-1.wav'\n",
    "print_prediction(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
