{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Evaluation\n",
    "# x_train -  train samples: 2185\n",
    "# x_test - testing samples: 547\n",
    "# categories train: 2185\n",
    "# total samples\n",
    "\n",
    "# Retrieve previously stored variables\n",
    "%store -r x_train \n",
    "%store -r x_test \n",
    "%store -r y_train\n",
    "%store -r y_test\n",
    "%store -r yy\n",
    "%store -r le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-121.22656   ,   65.431366  ,  -32.428158  ,   34.480045  ,\n",
       "        -20.285666  ,    6.766245  ,  -16.782026  ,   18.733065  ,\n",
       "        -13.358098  ,   11.913208  ,  -13.08202   ,   14.028469  ,\n",
       "        -17.593868  ,   13.480655  ,   -7.664562  ,    2.072071  ,\n",
       "         -2.1378267 ,    3.0446467 ,   -3.9934742 ,   10.67374   ,\n",
       "        -13.127217  ,   11.37405   ,    2.145628  ,   -1.9967248 ,\n",
       "         -0.800105  ,    3.9523966 ,   -4.335793  ,    2.2346418 ,\n",
       "         -2.9265692 ,    1.1134802 ,   -6.3341794 ,    0.1592586 ,\n",
       "         -4.7829237 ,   -2.4864097 ,   -5.620346  ,    4.3450346 ,\n",
       "         -9.871351  ,    2.3727033 ,   -0.61368215,    3.178931  ],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "(1346, 40)\n"
     ]
    }
   ],
   "source": [
    "display(x_train[1])\n",
    "print((y_train))\n",
    "display((yy.shape[1]))\n",
    "print(len(x_test))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "# Extract the number of labels - 4 in our case\n",
    "num_labels = yy.shape[1]\n",
    "\n",
    "# Begin constructing ML model\n",
    "# Create an object of the Sequential class\n",
    "model = Sequential()\n",
    "\n",
    "# Create input layer using the Dense function\n",
    "numNodes = 128; # Trial and error - 1/4 of 256 bit - went from 64 to 32 because size dropped from 2.7k to 1.4k\n",
    "numMFCC = 40;\n",
    "# Input shape is the size of the input array (1-D array of 40 columns, 1 row)\n",
    "model.add(Dense(numNodes, input_shape=(numMFCC,)));\n",
    "# Specifying the activation function to be used - relu: Rectified Linear Activiation\n",
    "model.add(Activation('relu'))\n",
    "# Dropout value of 50% - means random half of neurons exluded from each update cycle. Used to prevent overfitting.\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Add subsequest hidden layer - DENSE function\n",
    "model.add(Dense(numNodes))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add the output layer - DENSE function\n",
    "# Output nodes is the different categories\n",
    "# Different usage of activation function\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# Optimizer \"adam\" is a typical optimizer used - variation SGD (stochastic gradient descent)\n",
    "# SGD utilizes the gradient of the loss function with respects to the weight\n",
    "# loss -> typical loss function \n",
    "# metrics is output to be displayed (accuracy is the output of the loss function (?))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 128)               5248      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 22,276\n",
      "Trainable params: 22,276\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Pre-training accuracy: 21.3333%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[25.750243530273437, 0.2133333384990692]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "# Verbose - displays info if desired (verbose = 0 means silent, just print accuracy value)\n",
    "# evaluate returns loss value and score value\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "# Accuracy - the metrics value evaluated based on loss function\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)\n",
    "\n",
    "display(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1346 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "1346/1346 [==============================] - 1s 517us/step - loss: 20.8725 - accuracy: 0.3202 - val_loss: 2.5141 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.51408, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 2/100\n",
      "1346/1346 [==============================] - 0s 257us/step - loss: 6.0077 - accuracy: 0.4398 - val_loss: 0.8845 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.51408 to 0.88449, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 3/100\n",
      "1346/1346 [==============================] - 1s 426us/step - loss: 2.7505 - accuracy: 0.4844 - val_loss: 0.9295 - val_accuracy: 0.5933\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.88449\n",
      "Epoch 4/100\n",
      "1346/1346 [==============================] - 0s 294us/step - loss: 1.8739 - accuracy: 0.5059 - val_loss: 0.9351 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.88449\n",
      "Epoch 5/100\n",
      "1346/1346 [==============================] - 1s 407us/step - loss: 1.5272 - accuracy: 0.4993 - val_loss: 0.9377 - val_accuracy: 0.6533\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.88449\n",
      "Epoch 6/100\n",
      "1346/1346 [==============================] - 0s 303us/step - loss: 1.1604 - accuracy: 0.5327 - val_loss: 0.9356 - val_accuracy: 0.5933\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.88449\n",
      "Epoch 7/100\n",
      "1346/1346 [==============================] - 0s 298us/step - loss: 1.0755 - accuracy: 0.5498 - val_loss: 0.8968 - val_accuracy: 0.6267\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.88449\n",
      "Epoch 8/100\n",
      "1346/1346 [==============================] - 0s 354us/step - loss: 1.0075 - accuracy: 0.5877 - val_loss: 0.8638 - val_accuracy: 0.6400\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.88449 to 0.86383, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 9/100\n",
      "1346/1346 [==============================] - 0s 312us/step - loss: 1.0016 - accuracy: 0.5973 - val_loss: 0.8178 - val_accuracy: 0.6867\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.86383 to 0.81777, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 10/100\n",
      "1346/1346 [==============================] - 1s 411us/step - loss: 0.9058 - accuracy: 0.6100 - val_loss: 0.7730 - val_accuracy: 0.6867\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.81777 to 0.77296, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 11/100\n",
      "1346/1346 [==============================] - 1s 411us/step - loss: 0.8531 - accuracy: 0.6322 - val_loss: 0.7222 - val_accuracy: 0.7200\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.77296 to 0.72223, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 12/100\n",
      "1346/1346 [==============================] - 0s 272us/step - loss: 0.8661 - accuracy: 0.6382 - val_loss: 0.6927 - val_accuracy: 0.7267\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.72223 to 0.69274, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 13/100\n",
      "1346/1346 [==============================] - 0s 362us/step - loss: 0.8270 - accuracy: 0.6523 - val_loss: 0.6834 - val_accuracy: 0.7200\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.69274 to 0.68342, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 14/100\n",
      "1346/1346 [==============================] - 0s 270us/step - loss: 0.7622 - accuracy: 0.6761 - val_loss: 0.6385 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.68342 to 0.63851, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 15/100\n",
      "1346/1346 [==============================] - 0s 355us/step - loss: 0.7404 - accuracy: 0.6850 - val_loss: 0.6123 - val_accuracy: 0.7933\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.63851 to 0.61233, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 16/100\n",
      "1346/1346 [==============================] - 0s 273us/step - loss: 0.6845 - accuracy: 0.7080 - val_loss: 0.6265 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.61233\n",
      "Epoch 17/100\n",
      "1346/1346 [==============================] - 0s 326us/step - loss: 0.6740 - accuracy: 0.7125 - val_loss: 0.5992 - val_accuracy: 0.7733\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.61233 to 0.59918, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 18/100\n",
      "1346/1346 [==============================] - 0s 328us/step - loss: 0.6777 - accuracy: 0.7088 - val_loss: 0.5873 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.59918 to 0.58733, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 19/100\n",
      "1346/1346 [==============================] - 0s 328us/step - loss: 0.6339 - accuracy: 0.7221 - val_loss: 0.5880 - val_accuracy: 0.7933\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.58733\n",
      "Epoch 20/100\n",
      "1346/1346 [==============================] - 0s 302us/step - loss: 0.6283 - accuracy: 0.7244 - val_loss: 0.5859 - val_accuracy: 0.7867\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.58733 to 0.58586, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 21/100\n",
      "1346/1346 [==============================] - 0s 317us/step - loss: 0.6081 - accuracy: 0.7429 - val_loss: 0.6006 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.58586\n",
      "Epoch 22/100\n",
      "1346/1346 [==============================] - 0s 273us/step - loss: 0.6222 - accuracy: 0.7422 - val_loss: 0.5780 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.58586 to 0.57800, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 23/100\n",
      "1346/1346 [==============================] - 0s 290us/step - loss: 0.5533 - accuracy: 0.7600 - val_loss: 0.5586 - val_accuracy: 0.8267\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.57800 to 0.55859, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 24/100\n",
      "1346/1346 [==============================] - 1s 488us/step - loss: 0.5953 - accuracy: 0.7571 - val_loss: 0.5901 - val_accuracy: 0.8267\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.55859\n",
      "Epoch 25/100\n",
      "1346/1346 [==============================] - 0s 357us/step - loss: 0.5740 - accuracy: 0.7652 - val_loss: 0.5588 - val_accuracy: 0.8467\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.55859\n",
      "Epoch 26/100\n",
      "1346/1346 [==============================] - 0s 274us/step - loss: 0.5341 - accuracy: 0.7645 - val_loss: 0.5416 - val_accuracy: 0.8267\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.55859 to 0.54159, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 27/100\n",
      "1346/1346 [==============================] - 0s 306us/step - loss: 0.5541 - accuracy: 0.7712 - val_loss: 0.5427 - val_accuracy: 0.8067\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.54159\n",
      "Epoch 28/100\n",
      "1346/1346 [==============================] - 0s 339us/step - loss: 0.5219 - accuracy: 0.7927 - val_loss: 0.5813 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.54159\n",
      "Epoch 29/100\n",
      "1346/1346 [==============================] - 0s 339us/step - loss: 0.5212 - accuracy: 0.7912 - val_loss: 0.5278 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.54159 to 0.52783, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 30/100\n",
      "1346/1346 [==============================] - 1s 451us/step - loss: 0.4831 - accuracy: 0.7897 - val_loss: 0.5069 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.52783 to 0.50690, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 31/100\n",
      "1346/1346 [==============================] - 1s 396us/step - loss: 0.4641 - accuracy: 0.8076 - val_loss: 0.5525 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.50690\n",
      "Epoch 32/100\n",
      "1346/1346 [==============================] - 0s 295us/step - loss: 0.4795 - accuracy: 0.7972 - val_loss: 0.5264 - val_accuracy: 0.8067\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.50690\n",
      "Epoch 33/100\n",
      "1346/1346 [==============================] - 0s 313us/step - loss: 0.4837 - accuracy: 0.8105 - val_loss: 0.5164 - val_accuracy: 0.8067\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.50690\n",
      "Epoch 34/100\n",
      "1346/1346 [==============================] - 0s 332us/step - loss: 0.4169 - accuracy: 0.8224 - val_loss: 0.5229 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.50690\n",
      "Epoch 35/100\n",
      "1346/1346 [==============================] - 0s 319us/step - loss: 0.4510 - accuracy: 0.8247 - val_loss: 0.4877 - val_accuracy: 0.8467\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.50690 to 0.48768, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 36/100\n",
      "1346/1346 [==============================] - 1s 512us/step - loss: 0.4404 - accuracy: 0.8128 - val_loss: 0.5191 - val_accuracy: 0.8533\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.48768\n",
      "Epoch 37/100\n",
      "1346/1346 [==============================] - 1s 439us/step - loss: 0.3943 - accuracy: 0.8432 - val_loss: 0.5081 - val_accuracy: 0.8467\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.48768\n",
      "Epoch 38/100\n",
      "1346/1346 [==============================] - 0s 270us/step - loss: 0.4151 - accuracy: 0.8232 - val_loss: 0.5693 - val_accuracy: 0.8267\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.48768\n",
      "Epoch 39/100\n",
      "1346/1346 [==============================] - 1s 385us/step - loss: 0.4431 - accuracy: 0.8314 - val_loss: 0.5123 - val_accuracy: 0.8400\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.48768\n",
      "Epoch 40/100\n",
      "1346/1346 [==============================] - 0s 334us/step - loss: 0.4015 - accuracy: 0.8291 - val_loss: 0.4798 - val_accuracy: 0.8533\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.48768 to 0.47981, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 41/100\n",
      "1346/1346 [==============================] - 0s 354us/step - loss: 0.4162 - accuracy: 0.8351 - val_loss: 0.5033 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47981\n",
      "Epoch 42/100\n",
      "1346/1346 [==============================] - 0s 276us/step - loss: 0.4013 - accuracy: 0.8403 - val_loss: 0.5294 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47981\n",
      "Epoch 43/100\n",
      "1346/1346 [==============================] - 0s 351us/step - loss: 0.4005 - accuracy: 0.8403 - val_loss: 0.5720 - val_accuracy: 0.8267\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47981\n",
      "Epoch 44/100\n",
      "1346/1346 [==============================] - 0s 353us/step - loss: 0.4085 - accuracy: 0.8328 - val_loss: 0.5429 - val_accuracy: 0.8133\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47981\n",
      "Epoch 45/100\n",
      "1346/1346 [==============================] - 0s 298us/step - loss: 0.3769 - accuracy: 0.8432 - val_loss: 0.4847 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47981\n",
      "Epoch 46/100\n",
      "1346/1346 [==============================] - 0s 353us/step - loss: 0.3864 - accuracy: 0.8395 - val_loss: 0.4695 - val_accuracy: 0.8533\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.47981 to 0.46949, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 47/100\n",
      "1346/1346 [==============================] - 0s 291us/step - loss: 0.3751 - accuracy: 0.8470 - val_loss: 0.4549 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.46949 to 0.45487, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 48/100\n",
      "1346/1346 [==============================] - 0s 335us/step - loss: 0.3699 - accuracy: 0.8477 - val_loss: 0.5592 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.45487\n",
      "Epoch 49/100\n",
      "1346/1346 [==============================] - 0s 296us/step - loss: 0.3701 - accuracy: 0.8574 - val_loss: 0.4679 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.45487\n",
      "Epoch 50/100\n",
      "1346/1346 [==============================] - 0s 316us/step - loss: 0.3319 - accuracy: 0.8618 - val_loss: 0.4695 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.45487\n",
      "Epoch 51/100\n",
      "1346/1346 [==============================] - 0s 273us/step - loss: 0.3502 - accuracy: 0.8730 - val_loss: 0.5318 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.45487\n",
      "Epoch 52/100\n",
      "1346/1346 [==============================] - 0s 304us/step - loss: 0.3647 - accuracy: 0.8529 - val_loss: 0.5406 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.45487\n",
      "Epoch 53/100\n",
      "1346/1346 [==============================] - 0s 343us/step - loss: 0.3575 - accuracy: 0.8588 - val_loss: 0.5589 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.45487\n",
      "Epoch 54/100\n",
      "1346/1346 [==============================] - 0s 294us/step - loss: 0.3440 - accuracy: 0.8678 - val_loss: 0.4998 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.45487\n",
      "Epoch 55/100\n",
      "1346/1346 [==============================] - 0s 350us/step - loss: 0.3197 - accuracy: 0.8804 - val_loss: 0.5615 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.45487\n",
      "Epoch 56/100\n",
      "1346/1346 [==============================] - 0s 273us/step - loss: 0.3527 - accuracy: 0.8655 - val_loss: 0.5134 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.45487\n",
      "Epoch 57/100\n",
      "1346/1346 [==============================] - 0s 350us/step - loss: 0.3310 - accuracy: 0.8633 - val_loss: 0.5228 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.45487\n",
      "Epoch 58/100\n",
      "1346/1346 [==============================] - 0s 274us/step - loss: 0.3376 - accuracy: 0.8663 - val_loss: 0.4912 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.45487\n",
      "Epoch 59/100\n",
      "1346/1346 [==============================] - 0s 279us/step - loss: 0.3263 - accuracy: 0.8737 - val_loss: 0.5080 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.45487\n",
      "Epoch 60/100\n",
      "1346/1346 [==============================] - 0s 332us/step - loss: 0.3127 - accuracy: 0.8841 - val_loss: 0.5375 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.45487\n",
      "Epoch 61/100\n",
      "1346/1346 [==============================] - 0s 297us/step - loss: 0.3383 - accuracy: 0.8648 - val_loss: 0.4321 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.45487 to 0.43206, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 62/100\n",
      "1346/1346 [==============================] - 1s 383us/step - loss: 0.3542 - accuracy: 0.8603 - val_loss: 0.4377 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.43206\n",
      "Epoch 63/100\n",
      "1346/1346 [==============================] - 0s 274us/step - loss: 0.3361 - accuracy: 0.8737 - val_loss: 0.4014 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.43206 to 0.40137, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 64/100\n",
      "1346/1346 [==============================] - 0s 342us/step - loss: 0.3283 - accuracy: 0.8744 - val_loss: 0.5063 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.40137\n",
      "Epoch 65/100\n",
      "1346/1346 [==============================] - 0s 277us/step - loss: 0.3066 - accuracy: 0.8819 - val_loss: 0.5370 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.40137\n",
      "Epoch 66/100\n",
      "1346/1346 [==============================] - 0s 309us/step - loss: 0.2918 - accuracy: 0.8767 - val_loss: 0.5496 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.40137\n",
      "Epoch 67/100\n",
      "1346/1346 [==============================] - 0s 341us/step - loss: 0.3037 - accuracy: 0.8789 - val_loss: 0.5145 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.40137\n",
      "Epoch 68/100\n",
      "1346/1346 [==============================] - 0s 270us/step - loss: 0.3033 - accuracy: 0.8923 - val_loss: 0.4676 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.40137\n",
      "Epoch 69/100\n",
      "1346/1346 [==============================] - 0s 323us/step - loss: 0.2737 - accuracy: 0.9012 - val_loss: 0.5090 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.40137\n",
      "Epoch 70/100\n",
      "1346/1346 [==============================] - 0s 281us/step - loss: 0.2847 - accuracy: 0.8975 - val_loss: 0.5031 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.40137\n",
      "Epoch 71/100\n",
      "1346/1346 [==============================] - 1s 394us/step - loss: 0.3250 - accuracy: 0.8811 - val_loss: 0.4650 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.40137\n",
      "Epoch 72/100\n",
      "1346/1346 [==============================] - 0s 277us/step - loss: 0.3062 - accuracy: 0.8744 - val_loss: 0.4073 - val_accuracy: 0.8533\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.40137\n",
      "Epoch 73/100\n",
      "1346/1346 [==============================] - 0s 273us/step - loss: 0.2767 - accuracy: 0.8893 - val_loss: 0.4469 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.40137\n",
      "Epoch 74/100\n",
      "1346/1346 [==============================] - 0s 333us/step - loss: 0.3071 - accuracy: 0.8871 - val_loss: 0.4371 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.40137\n",
      "Epoch 75/100\n",
      "1346/1346 [==============================] - 0s 272us/step - loss: 0.3012 - accuracy: 0.8900 - val_loss: 0.4629 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.40137\n",
      "Epoch 76/100\n",
      "1346/1346 [==============================] - 1s 421us/step - loss: 0.2873 - accuracy: 0.8952 - val_loss: 0.4391 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.40137\n",
      "Epoch 77/100\n",
      "1346/1346 [==============================] - 1s 454us/step - loss: 0.2858 - accuracy: 0.8975 - val_loss: 0.4169 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.40137\n",
      "Epoch 78/100\n",
      "1346/1346 [==============================] - 0s 356us/step - loss: 0.2801 - accuracy: 0.8886 - val_loss: 0.4729 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.40137\n",
      "Epoch 79/100\n",
      "1346/1346 [==============================] - 0s 309us/step - loss: 0.2755 - accuracy: 0.8848 - val_loss: 0.4190 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.40137\n",
      "Epoch 80/100\n",
      "1346/1346 [==============================] - 0s 347us/step - loss: 0.2965 - accuracy: 0.8811 - val_loss: 0.4521 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.40137\n",
      "Epoch 81/100\n",
      "1346/1346 [==============================] - 0s 278us/step - loss: 0.2934 - accuracy: 0.8886 - val_loss: 0.4723 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.40137\n",
      "Epoch 82/100\n",
      "1346/1346 [==============================] - 0s 333us/step - loss: 0.3039 - accuracy: 0.8990 - val_loss: 0.4780 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.40137\n",
      "Epoch 83/100\n",
      "1346/1346 [==============================] - 0s 272us/step - loss: 0.2772 - accuracy: 0.8938 - val_loss: 0.4520 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.40137\n",
      "Epoch 84/100\n",
      "1346/1346 [==============================] - 0s 307us/step - loss: 0.2804 - accuracy: 0.9034 - val_loss: 0.4537 - val_accuracy: 0.8800\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.40137\n",
      "Epoch 85/100\n",
      "1346/1346 [==============================] - 0s 346us/step - loss: 0.2895 - accuracy: 0.8811 - val_loss: 0.4319 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.40137\n",
      "Epoch 86/100\n",
      "1346/1346 [==============================] - 0s 298us/step - loss: 0.2668 - accuracy: 0.8967 - val_loss: 0.4448 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.40137\n",
      "Epoch 87/100\n",
      "1346/1346 [==============================] - 0s 337us/step - loss: 0.2283 - accuracy: 0.9175 - val_loss: 0.4066 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.40137\n",
      "Epoch 88/100\n",
      "1346/1346 [==============================] - 0s 296us/step - loss: 0.2424 - accuracy: 0.9056 - val_loss: 0.4699 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.40137\n",
      "Epoch 89/100\n",
      "1346/1346 [==============================] - 0s 353us/step - loss: 0.2724 - accuracy: 0.9049 - val_loss: 0.4005 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.40137 to 0.40048, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 90/100\n",
      "1346/1346 [==============================] - 0s 303us/step - loss: 0.2691 - accuracy: 0.8997 - val_loss: 0.3889 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.40048 to 0.38895, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 91/100\n",
      "1346/1346 [==============================] - 1s 387us/step - loss: 0.2342 - accuracy: 0.9071 - val_loss: 0.3727 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.38895 to 0.37270, saving model to saved_models/weights.best.basic_mlp.hdf5\n",
      "Epoch 92/100\n",
      "1346/1346 [==============================] - 0s 278us/step - loss: 0.2606 - accuracy: 0.9012 - val_loss: 0.4346 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.37270\n",
      "Epoch 93/100\n",
      "1346/1346 [==============================] - 0s 311us/step - loss: 0.2276 - accuracy: 0.9153 - val_loss: 0.4712 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.37270\n",
      "Epoch 94/100\n",
      "1346/1346 [==============================] - 0s 316us/step - loss: 0.2856 - accuracy: 0.9123 - val_loss: 0.4248 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.37270\n",
      "Epoch 95/100\n",
      "1346/1346 [==============================] - 0s 280us/step - loss: 0.2518 - accuracy: 0.9012 - val_loss: 0.4985 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.37270\n",
      "Epoch 96/100\n",
      "1346/1346 [==============================] - 0s 348us/step - loss: 0.2538 - accuracy: 0.9034 - val_loss: 0.4681 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.37270\n",
      "Epoch 97/100\n",
      "1346/1346 [==============================] - 0s 281us/step - loss: 0.2548 - accuracy: 0.9086 - val_loss: 0.4947 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.37270\n",
      "Epoch 98/100\n",
      "1346/1346 [==============================] - 0s 368us/step - loss: 0.2527 - accuracy: 0.8982 - val_loss: 0.4617 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.37270\n",
      "Epoch 99/100\n",
      "1346/1346 [==============================] - 0s 293us/step - loss: 0.2473 - accuracy: 0.9027 - val_loss: 0.4650 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.37270\n",
      "Epoch 100/100\n",
      "1346/1346 [==============================] - 0s 297us/step - loss: 0.2471 - accuracy: 0.9019 - val_loss: 0.5488 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.37270\n",
      "Training completed in time:  0:00:47.391247\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 8 # Arbitrarily chose the value 8\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_mlp.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# Train the model for a fix number of epochs\n",
    "# validation_data - data to evaluate the loss at the end of each epoch\n",
    "# callbacks - display ModelCheckpoint\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  95.39375901222229 %\n",
      "Testing Accuracy:  89.33333158493042 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "\n",
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1]*100, \"%\")\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1]*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Creating a function that extracts the MFCC features of an audio file\n",
    "def extract_feature(file_name):\n",
    "   \n",
    "    try:\n",
    "        audio_data, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40)\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file)\n",
    "        return None, None\n",
    "\n",
    "    return np.array([mfccsscaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that will \n",
    "def print_prediction(file_name):\n",
    "    \n",
    "    # MFCCs of the specifc file contained in prediction_feature\n",
    "    prediction_feature = extract_feature(file_name)\n",
    "\n",
    "    # Directly maps to output\n",
    "    predicted_vector = model.predict_classes(prediction_feature)\n",
    "    \n",
    "    # Inverse transform is used to convert encoded LabelEncoder() values back to strings\n",
    "    predicted_class = le.inverse_transform(predicted_vector) \n",
    "    print(\"The predicted class is:\", predicted_class[0], '\\n') \n",
    "\n",
    "    # Given new instance, model return probability (of belonging to each class) between 0 and 1 \n",
    "    predicted_proba_vector = model.predict_proba(prediction_feature) \n",
    "    # Extract first array from array of arrays\n",
    "    predicted_proba = predicted_proba_vector[0]\n",
    "    \n",
    "    for i in range(len(predicted_proba)): \n",
    "        category = le.inverse_transform(np.array([i]))\n",
    "        print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-44216887abae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcwd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'//UrbanSound8K//audio//fold1//101415-3-0-2.wav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Siren file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8d3c919689f0>\u001b[0m in \u001b[0;36mprint_prediction\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Directly maps to output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpredicted_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Inverse transform is used to convert encoded LabelEncoder() values back to strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Validation \n",
    "import os\n",
    "\n",
    "# Random dog bark file\n",
    "cwd = os.getcwd()\n",
    "filename = cwd + '//UrbanSound8K//audio//fold1//101415-3-0-2.wav'\n",
    "print_prediction(filename)\n",
    "\n",
    "# Siren file\n",
    "cwd = os.getcwd()\n",
    "filename = cwd + '//UrbanSound8K//audio//fold3//184623-8-0-1.wav'\n",
    "print_prediction(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
